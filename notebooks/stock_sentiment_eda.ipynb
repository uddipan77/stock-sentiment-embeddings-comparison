{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock News Sentiment Analysis - Exploratory Data Analysis\n",
        "\n",
        "**Dataset**: Apple Stock News Articles with Sentiment Labels  \n",
        "**Period**: January 2019 - April 2019  \n",
        "**Features**: Date, News, Open, High, Low, Close, Volume, Label  \n",
        "**Labels**: -1 (Negative), 0 (Neutral), 1 (Positive)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis on stock news sentiment data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For text processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "%matplotlib inline\n",
        "\n",
        "print('✓ All libraries loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n",
        "\n",
        "**Important**: Update the file path to match your local setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION 1: If using the modular project structure\n",
        "# Uncomment and use this if you have the full project structure\n",
        "# import sys\n",
        "# from pathlib import Path\n",
        "# sys.path.append(str(Path.cwd().parent))\n",
        "# from src.data.data_loader import DataLoader\n",
        "# data_loader = DataLoader()\n",
        "# df = data_loader.load_data()\n",
        "\n",
        "# OPTION 2: Direct file path (RECOMMENDED for standalone notebook)\n",
        "# Update this path to match your file location\n",
        "file_path = r'C:\\pathto\\stock_news.csv'\n",
        "\n",
        "# Alternative: If file is in the same directory\n",
        "# file_path = 'stock_news.csv'\n",
        "\n",
        "# Alternative: If file is in parent directory\n",
        "# file_path = '../data/raw/stock_news.csv'\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f'✓ Dataset loaded successfully!')\n",
        "print(f'Shape: {df.shape[0]:,} rows × {df.shape[1]} columns')\n",
        "print(f'\\nFirst few rows:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('DATASET INFORMATION')\n",
        "print('='*80)\n",
        "print(f'\\nTotal Records: {len(df):,}')\n",
        "print(f'Number of Features: {len(df.columns)}')\n",
        "print(f'\\nColumn Names and Types:')\n",
        "for col in df.columns:\n",
        "    print(f'  {col:15s}: {str(df[col].dtype):12s} (Non-null: {df[col].count():,})')\n",
        "print(f'\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality assessment\n",
        "print('\\nDATA QUALITY CHECKS')\n",
        "print('='*80)\n",
        "missing_data = df.isnull().sum()\n",
        "if missing_data.sum() > 0:\n",
        "    print('\\nMissing Values:')\n",
        "    print(missing_data[missing_data > 0])\n",
        "else:\n",
        "    print('\\n✓ No missing values found!')\n",
        "\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f'\\nDuplicate Rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)')\n",
        "\n",
        "# Statistical summary\n",
        "print('\\nNumerical Features Summary:')\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sentiment Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment distribution\n",
        "label_counts = df['Label'].value_counts().sort_index()\n",
        "label_pct = (label_counts / len(df) * 100).round(2)\n",
        "\n",
        "label_map = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n",
        "label_names = [label_map[x] for x in label_counts.index]\n",
        "\n",
        "print('='*80)\n",
        "print('SENTIMENT DISTRIBUTION')\n",
        "print('='*80)\n",
        "for lbl in sorted(label_map.keys()):\n",
        "    if lbl in label_counts.index:\n",
        "        name = label_map[lbl]\n",
        "        count = label_counts[lbl]\n",
        "        pct = label_pct[lbl]\n",
        "        print(f'{name:10s} ({lbl:2d}): {count:5d} articles ({pct:5.2f}%)')\n",
        "\n",
        "# Check for class imbalance\n",
        "max_count = label_counts.max()\n",
        "min_count = label_counts.min()\n",
        "imbalance_ratio = max_count / min_count\n",
        "print(f'\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1')\n",
        "if imbalance_ratio > 2:\n",
        "    print('⚠️  Significant class imbalance detected!')\n",
        "    print('   → Consider using stratified sampling')\n",
        "    print('   → Use weighted metrics for evaluation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentiment distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "colors = ['#e74c3c', '#95a5a6', '#2ecc71']\n",
        "\n",
        "# Bar chart\n",
        "bars = axes[0].bar(label_names, label_counts.values, color=colors, \n",
        "                  alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[0].set_xlabel('Sentiment', fontweight='bold', fontsize=13)\n",
        "axes[0].set_ylabel('Number of Articles', fontweight='bold', fontsize=13)\n",
        "axes[0].set_title('Sentiment Distribution (Counts)', fontweight='bold', fontsize=14)\n",
        "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height):,}\\n({height/len(df)*100:.1f}%)',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "# Pie chart\n",
        "wedges, texts, autotexts = axes[1].pie(label_counts.values, labels=label_names, \n",
        "                                       autopct='%1.1f%%', colors=colors, \n",
        "                                       startangle=90, explode=(0.05, 0.05, 0.05),\n",
        "                                       shadow=True, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
        "axes[1].set_title('Sentiment Distribution (Percentage)', fontweight='bold', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert date and analyze temporal patterns\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print('TEMPORAL COVERAGE')\n",
        "print('='*80)\n",
        "print(f\"Start Date: {df['Date'].min().strftime('%B %d, %Y')}\")\n",
        "print(f\"End Date:   {df['Date'].max().strftime('%B %d, %Y')}\")\n",
        "print(f\"Duration:   {(df['Date'].max() - df['Date'].min()).days} days\")\n",
        "print(f\"\\nArticles per Day (avg): {len(df) / (df['Date'].max() - df['Date'].min()).days:.1f}\")\n",
        "\n",
        "# Articles per day\n",
        "daily_counts = df.groupby('Date').size()\n",
        "print(f\"\\nDaily Article Statistics:\")\n",
        "print(f\"  Min:    {daily_counts.min():3d} articles\")\n",
        "print(f\"  Max:    {daily_counts.max():3d} articles\")\n",
        "print(f\"  Mean:   {daily_counts.mean():6.1f} articles\")\n",
        "print(f\"  Median: {daily_counts.median():6.1f} articles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment over time\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Sentiment distribution over time\n",
        "sentiment_time = df.groupby(['Date', 'Label']).size().unstack(fill_value=0)\n",
        "sentiment_time.plot(kind='area', ax=axes[0], color=colors, alpha=0.7, stacked=True)\n",
        "axes[0].set_xlabel('Date', fontweight='bold', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Articles', fontweight='bold', fontsize=12)\n",
        "axes[0].set_title('News Articles Over Time (by Sentiment)', fontweight='bold', fontsize=14)\n",
        "axes[0].legend(['Negative', 'Neutral', 'Positive'], loc='upper left', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Daily article volume\n",
        "daily_counts.plot(kind='bar', ax=axes[1], color='steelblue', alpha=0.7, width=0.8)\n",
        "axes[1].set_xlabel('Date', fontweight='bold', fontsize=12)\n",
        "axes[1].set_ylabel('Number of Articles', fontweight='bold', fontsize=12)\n",
        "axes[1].set_title('Daily News Volume', fontweight='bold', fontsize=14)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stock Price Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stock price statistics\n",
        "print('STOCK PRICE STATISTICS')\n",
        "print('='*80)\n",
        "print(f\"\\nPrice Range:\")\n",
        "print(f\"  Lowest Close:  ${df['Close'].min():.2f}\")\n",
        "print(f\"  Highest Close: ${df['Close'].max():.2f}\")\n",
        "print(f\"  Mean Close:    ${df['Close'].mean():.2f}\")\n",
        "print(f\"  Price Change:  ${df['Close'].max() - df['Close'].min():.2f}\")\n",
        "\n",
        "print(f\"\\nVolume Statistics:\")\n",
        "print(f\"  Mean Volume:   {df['Volume'].mean():,.0f}\")\n",
        "print(f\"  Max Volume:    {df['Volume'].max():,.0f}\")\n",
        "\n",
        "# Calculate returns\n",
        "df['Daily_Return'] = df['Close'].pct_change() * 100\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "\n",
        "print(f\"\\nReturns Analysis:\")\n",
        "print(f\"  Mean Daily Return: {df['Daily_Return'].mean():.2f}%\")\n",
        "print(f\"  Volatility (Std):  {df['Daily_Return'].std():.2f}%\")\n",
        "print(f\"  Max Gain:          {df['Daily_Return'].max():.2f}%\")\n",
        "print(f\"  Max Loss:          {df['Daily_Return'].min():.2f}%\")\n",
        "\n",
        "df[['Open', 'High', 'Low', 'Close', 'Volume']].describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot stock prices\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "# Price trends\n",
        "axes[0].plot(df['Date'], df['Open'], label='Open', alpha=0.6, linewidth=1.5)\n",
        "axes[0].plot(df['Date'], df['High'], label='High', alpha=0.6, linewidth=1.5)\n",
        "axes[0].plot(df['Date'], df['Low'], label='Low', alpha=0.6, linewidth=1.5)\n",
        "axes[0].plot(df['Date'], df['Close'], label='Close', alpha=0.9, linewidth=2.5, color='navy')\n",
        "axes[0].set_ylabel('Price ($)', fontweight='bold', fontsize=12)\n",
        "axes[0].set_title('Apple Stock Price Trends (Jan-Apr 2019)', fontweight='bold', fontsize=14)\n",
        "axes[0].legend(loc='best', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Volume\n",
        "axes[1].bar(df['Date'], df['Volume'], color='steelblue', alpha=0.6, width=0.8)\n",
        "axes[1].set_ylabel('Volume', fontweight='bold', fontsize=12)\n",
        "axes[1].set_title('Trading Volume', fontweight='bold', fontsize=14)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "# Daily returns\n",
        "colors_returns = ['red' if x < 0 else 'green' for x in df['Daily_Return']]\n",
        "axes[2].bar(df['Date'], df['Daily_Return'], color=colors_returns, alpha=0.6, width=0.8)\n",
        "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "axes[2].set_xlabel('Date', fontweight='bold', fontsize=12)\n",
        "axes[2].set_ylabel('Daily Return (%)', fontweight='bold', fontsize=12)\n",
        "axes[2].set_title('Daily Returns', fontweight='bold', fontsize=14)\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text statistics\n",
        "df['text_length'] = df['News'].apply(lambda x: len(str(x)))\n",
        "df['word_count'] = df['News'].apply(lambda x: len(str(x).split()))\n",
        "df['avg_word_len'] = df['News'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "print('TEXT STATISTICS')\n",
        "print('='*80)\n",
        "print(f\"Average Characters per Article: {df['text_length'].mean():.0f}\")\n",
        "print(f\"Average Words per Article:      {df['word_count'].mean():.0f}\")\n",
        "print(f\"Average Word Length:            {df['avg_word_len'].mean():.2f} chars\")\n",
        "\n",
        "print(f\"\\nText Length Range:\")\n",
        "print(f\"  Shortest: {df['text_length'].min():4d} chars ({df['word_count'].min():3d} words)\")\n",
        "print(f\"  Longest:  {df['text_length'].max():4d} chars ({df['word_count'].max():3d} words)\")\n",
        "\n",
        "print(f\"\\nDetailed Statistics:\")\n",
        "df[['text_length', 'word_count', 'avg_word_len']].describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Character distribution\n",
        "axes[0,0].hist(df['text_length'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0,0].axvline(df['text_length'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0,0].axvline(df['text_length'].median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
        "axes[0,0].set_xlabel('Characters', fontweight='bold')\n",
        "axes[0,0].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[0,0].set_title('Character Count Distribution', fontweight='bold')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Word distribution\n",
        "axes[0,1].hist(df['word_count'], bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[0,1].axvline(df['word_count'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0,1].axvline(df['word_count'].median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
        "axes[0,1].set_xlabel('Words', fontweight='bold')\n",
        "axes[0,1].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[0,1].set_title('Word Count Distribution', fontweight='bold')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# By sentiment\n",
        "for label, name, color in zip([-1, 0, 1], ['Negative', 'Neutral', 'Positive'], colors):\n",
        "    if label in df['Label'].unique():\n",
        "        subset = df[df['Label'] == label]['text_length']\n",
        "        axes[1,0].hist(subset, bins=30, alpha=0.5, label=name, color=color)\n",
        "axes[1,0].set_xlabel('Characters', fontweight='bold')\n",
        "axes[1,0].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[1,0].set_title('Text Length by Sentiment', fontweight='bold')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "data_by_label = [df[df['Label']==l]['word_count'].values for l in sorted(df['Label'].unique())]\n",
        "bp = axes[1,1].boxplot(data_by_label, labels=label_names, patch_artist=True)\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.6)\n",
        "axes[1,1].set_ylabel('Word Count', fontweight='bold')\n",
        "axes[1,1].set_title('Word Count Distribution by Sentiment', fontweight='bold')\n",
        "axes[1,1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Word Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text preprocessing function\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and tokenize text\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Process all text\n",
        "print('Processing text for word frequency analysis...')\n",
        "all_words = []\n",
        "label_words = {label: [] for label in df['Label'].unique()}\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    tokens = clean_text(row['News'])\n",
        "    all_words.extend(tokens)\n",
        "    label_words[row['Label']].extend(tokens)\n",
        "\n",
        "print(f'\\n✓ Processing complete!')\n",
        "print(f'Total words (after cleaning): {len(all_words):,}')\n",
        "print(f'Unique words: {len(set(all_words)):,}')\n",
        "print(f'Vocabulary richness: {len(set(all_words))/len(all_words)*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top words\n",
        "word_freq = Counter(all_words)\n",
        "top_30 = word_freq.most_common(30)\n",
        "\n",
        "print('\\nTOP 30 MOST COMMON WORDS')\n",
        "print('='*80)\n",
        "for i, (word, count) in enumerate(top_30, 1):\n",
        "    print(f'{i:2d}. {word:20s}: {count:6d} occurrences')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize word frequencies\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Bar chart - top 20\n",
        "top_20 = word_freq.most_common(20)\n",
        "words, counts = zip(*top_20)\n",
        "y_pos = np.arange(len(words))\n",
        "\n",
        "axes[0].barh(y_pos, counts, color='steelblue', alpha=0.8, edgecolor='black')\n",
        "axes[0].set_yticks(y_pos)\n",
        "axes[0].set_yticklabels(words)\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_xlabel('Frequency', fontweight='bold', fontsize=12)\n",
        "axes[0].set_title('Top 20 Most Common Words', fontweight='bold', fontsize=14)\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Word cloud\n",
        "wc = WordCloud(width=800, height=500, background_color='white',\n",
        "              colormap='viridis', max_words=100).generate(' '.join(all_words))\n",
        "axes[1].imshow(wc, interpolation='bilinear')\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Word Cloud (All Text)', fontweight='bold', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word clouds by sentiment\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "for idx, (label, name, color) in enumerate(zip([-1, 0, 1], \n",
        "                                               ['Negative', 'Neutral', 'Positive'],\n",
        "                                               ['Reds', 'Greys', 'Greens'])):\n",
        "    if label in label_words and len(label_words[label]) > 0:\n",
        "        wc = WordCloud(width=600, height=400, background_color='white',\n",
        "                      colormap=color, max_words=80).generate(' '.join(label_words[label]))\n",
        "        axes[idx].imshow(wc, interpolation='bilinear')\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(f'{name} Sentiment', fontweight='bold', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "num_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Label', \n",
        "           'text_length', 'word_count', 'Daily_Return']\n",
        "num_cols = [col for col in num_cols if col in df.columns]\n",
        "corr = df[num_cols].corr()\n",
        "\n",
        "print('CORRELATION ANALYSIS')\n",
        "print('='*80)\n",
        "print('\\nCorrelation Matrix:')\n",
        "print(corr.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize correlation\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f',\n",
        "           square=True, linewidths=1.5, cbar_kws={'label': 'Correlation Coefficient'},\n",
        "           mask=mask, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap', fontweight='bold', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# High correlations\n",
        "print('\\nHigh Correlations (|r| > 0.7):')\n",
        "print('='*80)\n",
        "found = False\n",
        "for i in range(len(corr.columns)):\n",
        "    for j in range(i+1, len(corr.columns)):\n",
        "        if abs(corr.iloc[i,j]) > 0.7:\n",
        "            print(f'{corr.columns[i]:15s} <-> {corr.columns[j]:15s}: {corr.iloc[i,j]:6.3f}')\n",
        "            found = True\n",
        "if not found:\n",
        "    print('No strong correlations (|r| > 0.7) found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Sentiment-Price Relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate by sentiment\n",
        "sent_stats = df.groupby('Label').agg({\n",
        "    'Close': ['mean', 'std', 'min', 'max'],\n",
        "    'Volume': ['mean', 'std'],\n",
        "    'Daily_Return': ['mean', 'std']\n",
        "}).round(2)\n",
        "\n",
        "print('STOCK METRICS BY SENTIMENT')\n",
        "print('='*80)\n",
        "print(sent_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentiment-price relationship\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Avg close price\n",
        "avg_close = df.groupby('Label')['Close'].mean()\n",
        "bars = axes[0,0].bar(label_names, [avg_close[l] for l in sorted(df['Label'].unique())],\n",
        "                    color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[0,0].set_ylabel('Average Closing Price ($)', fontweight='bold', fontsize=12)\n",
        "axes[0,0].set_title('Average Closing Price by Sentiment', fontweight='bold', fontsize=13)\n",
        "axes[0,0].grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                  f'${height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Avg volume\n",
        "avg_vol = df.groupby('Label')['Volume'].mean()\n",
        "bars = axes[0,1].bar(label_names, [avg_vol[l] for l in sorted(df['Label'].unique())],\n",
        "                    color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[0,1].set_ylabel('Average Volume', fontweight='bold', fontsize=12)\n",
        "axes[0,1].set_title('Average Trading Volume by Sentiment', fontweight='bold', fontsize=13)\n",
        "axes[0,1].grid(axis='y', alpha=0.3)\n",
        "axes[0,1].ticklabel_format(style='plain', axis='y')\n",
        "\n",
        "# Price change\n",
        "avg_chg = df.groupby('Label')['Price_Change'].mean()\n",
        "bars = axes[1,0].bar(label_names, [avg_chg[l] for l in sorted(df['Label'].unique())],\n",
        "                    color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[1,0].axhline(0, color='black', linewidth=1.5)\n",
        "axes[1,0].set_ylabel('Average Price Change ($)', fontweight='bold', fontsize=12)\n",
        "axes[1,0].set_title('Average Intraday Price Change by Sentiment', fontweight='bold', fontsize=13)\n",
        "axes[1,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Daily return\n",
        "avg_ret = df.groupby('Label')['Daily_Return'].mean()\n",
        "bars = axes[1,1].bar(label_names, [avg_ret[l] for l in sorted(df['Label'].unique())],\n",
        "                    color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[1,1].axhline(0, color='black', linewidth=1.5)\n",
        "axes[1,1].set_ylabel('Average Daily Return (%)', fontweight='bold', fontsize=12)\n",
        "axes[1,1].set_title('Average Daily Return by Sentiment', fontweight='bold', fontsize=13)\n",
        "axes[1,1].grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                  f'{height:.2f}%', ha='center', \n",
        "                  va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Sample Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample articles\n",
        "print('='*80)\n",
        "print('SAMPLE NEWS ARTICLES BY SENTIMENT')\n",
        "print('='*80)\n",
        "\n",
        "for label, name in label_map.items():\n",
        "    if label in df['Label'].unique():\n",
        "        print(f'\\n{name.upper()} SENTIMENT ({label:+d})')\n",
        "        print('-' * 80)\n",
        "        samples = df[df['Label'] == label].sample(min(3, len(df[df['Label'] == label])))\n",
        "        for i, (idx, row) in enumerate(samples.iterrows(), 1):\n",
        "            print(f\"\\n[{i}] Date: {row['Date'].strftime('%Y-%m-%d')}\")\n",
        "            print(f\"    Stock: Close=${row['Close']:.2f}, Volume={row['Volume']:,}\")\n",
        "            news_text = row['News'][:250] + '...' if len(row['News']) > 250 else row['News']\n",
        "            print(f\"    News: {news_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Key Insights & Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS')\n",
        "print('='*80)\n",
        "\n",
        "print(f'\\n1. DATASET OVERVIEW')\n",
        "print(f'   • Total articles: {len(df):,}')\n",
        "print(f'   • Time period: {(df[\"Date\"].max()-df[\"Date\"].min()).days} days '\n",
        "      f'({df[\"Date\"].min().strftime(\"%b %Y\")} to {df[\"Date\"].max().strftime(\"%b %Y\")})')\n",
        "print(f'   • Daily volume: {len(df)/(df[\"Date\"].max()-df[\"Date\"].min()).days:.1f} articles/day')\n",
        "\n",
        "print(f'\\n2. SENTIMENT DISTRIBUTION')\n",
        "for l in sorted(label_map.keys()):\n",
        "    if l in label_counts.index:\n",
        "        print(f'   • {label_map[l]:8s}: {label_counts[l]:4d} ({label_pct[l]:5.1f}%)')\n",
        "print(f'   • Imbalance ratio: {imbalance_ratio:.2f}:1 '\n",
        "      f'({\"⚠️ Significant\" if imbalance_ratio > 2 else \"✓ Balanced\"})')\n",
        "\n",
        "print(f'\\n3. TEXT CHARACTERISTICS')\n",
        "print(f'   • Avg length: {df[\"word_count\"].mean():.0f} words ({df[\"text_length\"].mean():.0f} chars)')\n",
        "print(f'   • Vocabulary: {len(set(all_words)):,} unique words')\n",
        "print(f'   • Top keyword: \"{word_freq.most_common(1)[0][0]}\" ({word_freq.most_common(1)[0][1]:,} times)')\n",
        "\n",
        "print(f'\\n4. STOCK PRICE INSIGHTS')\n",
        "print(f'   • Price range: ${df[\"Close\"].min():.2f} - ${df[\"Close\"].max():.2f}')\n",
        "print(f'   • Mean price: ${df[\"Close\"].mean():.2f}')\n",
        "print(f'   • Avg daily return: {df[\"Daily_Return\"].mean():.2f}%')\n",
        "print(f'   • Volatility (σ): {df[\"Daily_Return\"].std():.2f}%')\n",
        "\n",
        "print(f'\\n5. SENTIMENT-PRICE RELATIONSHIP')\n",
        "for l in sorted(df['Label'].unique()):\n",
        "    name = label_map[l]\n",
        "    avg_ret = df[df['Label'] == l]['Daily_Return'].mean()\n",
        "    avg_price = df[df['Label'] == l]['Close'].mean()\n",
        "    print(f'   • {name:8s}: Avg return={avg_ret:+6.2f}%, Avg price=${avg_price:.2f}')\n",
        "\n",
        "print(f'\\n6. DATA QUALITY')\n",
        "print(f'   • Missing values: {df.isnull().sum().sum()}')\n",
        "print(f'   • Duplicates: {df.duplicated().sum()}')\n",
        "print(f'   • Data completeness: {(1 - df.isnull().sum().sum()/(len(df)*len(df.columns)))*100:.1f}%')\n",
        "print(f'   • Quality: ✓ GOOD' if df.isnull().sum().sum() == 0 else '   • Quality: ⚠️ NEEDS ATTENTION')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('✅ EXPLORATORY DATA ANALYSIS COMPLETED SUCCESSFULLY')\n",
        "print('='*80)\n",
        "print('\\nNext Steps:')\n",
        "print('  1. Preprocess text data (tokenization, stopword removal, lemmatization)')\n",
        "print('  2. Split data into train/validation/test sets (70/15/15)')\n",
        "print('  3. Generate embeddings (Word2Vec, GloVe, FastText, Sentence Transformers)')\n",
        "print('  4. Train Random Forest models with hyperparameter tuning')\n",
        "print('  5. Evaluate and compare model performances')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Song-Prediction_311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
